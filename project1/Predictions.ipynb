{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_train, tx_train_raw, ids_train) = (np.array(x) for x in load_csv_data(\"data/train.csv\"))\n",
    "(y_test, tx_test_raw, ids_test) = (np.array(x) for x in load_csv_data(\"data/test.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noisy_column(x):\n",
    "    \"\"\"Remove columns for which there are more -999 entries than normal entries\"\"\"\n",
    "    return np.array([i for i in x.T if (i == -999).sum() < (i != -999).sum()]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_noise_entries(x):\n",
    "    \"\"\"Replace -999 by mean value of the column (mean computed without any -999)\"\"\"\n",
    "    col_means = [np.mean(col[col!= -999]) for col in x.T]\n",
    "    x_local = x.copy()\n",
    "    for i,col in enumerate(x_local.T):\n",
    "        col[col == -999] = col_means[i]\n",
    "    return x_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outliers(x, mean, std):\n",
    "    outlier_indices = np.zeros() \n",
    "    for feature in range(tx.shape[1]):\n",
    "        row_indices = np.where(np.absolute(tx[:,feature]-mean[feature]) > 3*std[feature])[0]\n",
    "        mask = np.in1d(row_indices, outlier_indices)\n",
    "        outlier_indices = np.hstack((outlier_indices, row_indices[np.where(~mask)[0]]))\n",
    "    \n",
    "    return outlier_indices.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(w, x_test, y_test):\n",
    "    \"\"\"returns accuracy for a specific weight vector\"\"\"\n",
    "    predictions = predict_labels(w, x_test)\n",
    "    num_equal = (predictions == y_test).sum()\n",
    "    return num_equals/y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(x, num_important, degree):\n",
    "\n",
    "    for i, line in enumerate(x):\n",
    "        ones = np.array([1])\n",
    "#         for val in itertools.combinations(line[:num_important], r=degree):\n",
    "#             print(val)\n",
    "#         comb = np.array([np.prod(val) for val in itertools.combinations(line[:num_important], r=degree)])\n",
    "        not_imp = np.array([line**d for d in range(2, degree+1)])\n",
    "        complete = np.concatenate((ones, line, comb, not_imp), axis=None)\n",
    "        print(ones, line, comb, not_imp, complete)\n",
    "        if i == 0:\n",
    "            new = np.zeros((x.shape[0], complete.shape[0]))\n",
    "        new[i] = complete\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([[1,2,3,4],[2,2,2,2],[5,5,5,5], [3,2,3,2], [2,2,4,4]])\n",
    "# np.hstack()\n",
    "augment(t, 3, 3)\n",
    "# t2 = np.array([1,2,3,4,5])\n",
    "# print(np.power(t2, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y, tx, w):\n",
    "    return np.sum(np.power(y - np.dot(tx, w), 2)/(2*len(y)))  # MSE\n",
    "\n",
    "\n",
    "def MAE(y, tx, w):\n",
    "    return np.sum(np.abs(y - np.dot(tx, w)))/len(y)  # MAE\n",
    "\n",
    "\n",
    "def RMSE(y, tx, w):\n",
    "    return np.sqrt(2*MSE(y, tx, w))\n",
    "\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(e)/len(y)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def calculate_gradient_log(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    grad = tx.T.dot(pred - y)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1/(1+np.exp(-t))\n",
    "\n",
    "\n",
    "def NLL(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    loss = y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n",
    "    return np.squeeze(- loss)\n",
    "########################\n",
    "###### ASSIGNMENT ######\n",
    "########################\n",
    "\n",
    "\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        loss = MSE(y, tx, w)\n",
    "        if n_iter % 100 == 0:\n",
    "            print(loss)\n",
    "        w = w - gamma*grad\n",
    "        # print(\"Step {}, loss is   {}\".format(n_iter, loss))\n",
    "    return (w, loss)\n",
    "\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    weights = initial_w\n",
    "    for _ in range(max_iters):\n",
    "        rand_index = np.random.randint(y.shape)\n",
    "        y_batch, tx_batch = y[rand_index], tx[rand_index]\n",
    "        grad = compute_gradient(y_batch, tx_batch, weights)\n",
    "        weights = weights - gamma*grad\n",
    "    loss = MSE(y, tx, weights)\n",
    "    return (weights, loss)\n",
    "\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    loss = MSE(y, tx, w)\n",
    "    return (w, loss)\n",
    "\n",
    "\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    aI = 2 * tx.shape[0] * lambda_ * np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    loss = MSE(y, tx, w)\n",
    "    return (w, loss)\n",
    "\n",
    "\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    pass    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    def sigmoid(t):\n",
    "        \"\"\"apply sigmoid function on t.\"\"\"\n",
    "        return 1/(1+np.exp(-t))\n",
    "\n",
    "    def calculate_gradient(gradient_y, gradient_tx, gradient_w):\n",
    "        \"\"\"compute the gradient of loss.\"\"\"\n",
    "        pred = sigmoid(gradient_tx.dot(gradient_w))\n",
    "        grad = gradient_tx.T.dot(pred - gradient_y)\n",
    "        return grad\n",
    "\n",
    "    def calculate_loss(y, loss_tx, loss_w):\n",
    "        \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "        pred = sigmoid(loss_tx.dot(loss_w))\n",
    "        loss = y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n",
    "        return np.squeeze(- loss)\n",
    "\n",
    "    weights = initial_w\n",
    "    for _ in range(max_iters):\n",
    "        # get loss and update w.\n",
    "        loss = calculate_loss(y, tx, weights)\n",
    "        print(loss)\n",
    "        grad = calculate_gradient(y, tx, weights)\n",
    "        weights = weights - gamma * grad\n",
    "    return (weights, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y, tx, ids) = (np.array(x) for x in load_csv_data(\"data/train.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We quickly saw that the data is split into 4 categories, according to the `PRI_jet_num` column. We split according to these categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT_COL = 22\n",
    "NUM_CATEGORIES = 4\n",
    "rows_per_cat = np.array([np.where(tx[:,CAT_COL] == c)[0] for c in range(NUM_CATEGORIES)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this represents, for each category, which column contain at least one unknown (-999) value\n",
    "unknown_cols = [set(np.where(tx[np.where(tx[:, CAT_COL] == c)[0], :] == -999)[1]) for c in range(NUM_CATEGORIES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CATEGORY 0:\n",
      "Col 0 has 26.145746799715752% of unknown\n",
      "Col 4 has 100.0% of unknown\n",
      "Col 5 has 100.0% of unknown\n",
      "Col 6 has 100.0% of unknown\n",
      "Col 12 has 100.0% of unknown\n",
      "Col 23 has 100.0% of unknown\n",
      "Col 24 has 100.0% of unknown\n",
      "Col 25 has 100.0% of unknown\n",
      "Col 26 has 100.0% of unknown\n",
      "Col 27 has 100.0% of unknown\n",
      "Col 28 has 100.0% of unknown\n",
      "\n",
      "CATEGORY 1:\n",
      "Col 0 has 9.751882802022077% of unknown\n",
      "Col 4 has 100.0% of unknown\n",
      "Col 5 has 100.0% of unknown\n",
      "Col 6 has 100.0% of unknown\n",
      "Col 12 has 100.0% of unknown\n",
      "Col 26 has 100.0% of unknown\n",
      "Col 27 has 100.0% of unknown\n",
      "Col 28 has 100.0% of unknown\n",
      "\n",
      "CATEGORY 2:\n",
      "Col 0 has 5.859584350622283% of unknown\n",
      "\n",
      "CATEGORY 3:\n",
      "Col 0 has 6.663959574084101% of unknown\n"
     ]
    }
   ],
   "source": [
    "for i, (rows, cols) in enumerate(zip(rows_per_cat, unknown_cols)):\n",
    "    percentages = np.asarray([len(np.where(tx[rows, i] == -999)[0]) \\\n",
    "                             /len(rows) for i in cols]) * 100\n",
    "    print(\"\\nCATEGORY {}:\".format(i))\n",
    "    for col, perc in zip(cols, percentages):\n",
    "        print(\"Col {} has {}% of unknown\".format(col, perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, for every category, any column (except 0) that has any unknown value has only unknown values. This can be explained by the fact that some fields might not be relevant to a certain category, and thus filled with \"NaN\", or -999\n",
    "\n",
    "From now on, for all categories, we only keep columns with values different that -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in unknown_cols:\n",
    "    cat.remove(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove columns PHI, as they are identically distributed and provide no real meaning.\n",
    "We also remove row 22, as data is now split according to it.\n",
    "As categories 2 and 3 are basically the same, we consider them as identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_cols = [15, 18, 20, 25, 28]\n",
    "columns_to_remove = [np.unique(np.concatenate((list(unknown), [CAT_COL], phi_cols))) for unknown in unknown_cols]\n",
    "columns_to_remove = columns_to_remove[:-1]\n",
    "NUM_CATEGORIES = 3\n",
    "rows_per_cat[2] = np.unique(np.concatenate((rows_per_cat[2],rows_per_cat[3])))\n",
    "rows_per_cat = rows_per_cat[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_by_cat = [tx[rows] for rows in rows_per_cat]\n",
    "y_by_cat = [y[rows] for rows in rows_per_cat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_by_cat = [np.delete(tx_by_cat[cat], np.array(columns_to_remove[cat], dtype=int), axis=1) for cat in range(NUM_CATEGORIES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26123\n",
      "7562\n",
      "4429\n"
     ]
    }
   ],
   "source": [
    "# Replacing remaining NaN by the average value of the column for the category\n",
    "for cat in tx_by_cat:\n",
    "    first_col = cat[:,0]\n",
    "    first_col[first_col == -999] = np.mean(first_col[first_col != -999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
