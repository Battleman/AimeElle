{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from implementations import *\n",
    "# import numpy as np\n",
    "from proj1_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "####### IMPORTS ########\n",
    "########################\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "########################\n",
    "######## HELPERS #######\n",
    "########################\n",
    "def MSE(y, tx, w):\n",
    "    return np.sum(np.power(y - np.dot(tx, w), 2)/(2*len(y)))  # MSE\n",
    "\n",
    "\n",
    "def MAE(y, tx, w):\n",
    "    return np.sum(np.abs(y - np.dot(tx, w)))/len(y)  # MAE\n",
    "\n",
    "\n",
    "def RMSE(y, tx, w):\n",
    "    return np.sqrt(2*MSE(y, tx, w))\n",
    "\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(e)/len(y)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def calculate_gradient_log(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    grad = tx.T.dot(pred - y)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1/(1+np.exp(-t))\n",
    "\n",
    "\n",
    "def NLL(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    loss = y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n",
    "    return np.squeeze(- loss)\n",
    "########################\n",
    "###### ASSIGNMENT ######\n",
    "########################\n",
    "\n",
    "\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        loss = MSE(y, tx, w)\n",
    "        w = w - gamma*grad\n",
    "        # print(\"Step {}, loss is   {}\".format(n_iter, loss))\n",
    "    return (w, loss)\n",
    "\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size):\n",
    "            grad = compute_gradient(y, tx, w)\n",
    "            w = w - gamma*grad\n",
    "    loss = MSE(y, tx, w)\n",
    "    return (w, loss)\n",
    "\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    loss = MSE(y, tx, w)\n",
    "    return (w, loss)\n",
    "\n",
    "\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    aI = 2 * tx.shape[0] * lambda_ * np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    loss = MSE(y, tx, w)\n",
    "    return (w, loss)\n",
    "\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    def sigmoid(t):\n",
    "        \"\"\"apply sigmoid function on t.\"\"\"\n",
    "        return 1/(1+np.exp(-t))\n",
    "\n",
    "    def calculate_gradient(y, tx, w):\n",
    "        \"\"\"compute the gradient of loss.\"\"\"\n",
    "        pred = sigmoid(tx.dot(w))\n",
    "        print(\"pred: {}\".format(pred))\n",
    "        grad = tx.T.dot(pred - y)\n",
    "        return grad\n",
    "\n",
    "    def calculate_loss(y, tx, w):\n",
    "        \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "        pred = sigmoid(tx.dot(w))\n",
    "        loss = y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n",
    "        return np.squeeze(- loss)\n",
    "\n",
    "    def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "        \"\"\"\n",
    "        Do one step of gradient descen using logistic regression.\n",
    "        Return the loss and the updated w.\n",
    "        \"\"\"\n",
    "        loss = calculate_loss(y, tx, w)\n",
    "        grad = calculate_gradient(y, tx, w)\n",
    "        w2 = w - gamma * grad\n",
    "#         print(\"W went from {}\\nto {}\\nwith grad {}\".format(w[:5], w2[:5], grad[:5]))\n",
    "        return loss, w2\n",
    "\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for i in range(max_iters):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        # converge criterion\n",
    "        print(\"Step {}, loss is {}\".format(i, loss))\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < 1e-8:\n",
    "            break\n",
    "    return (w, loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_train, tX_train, ids_train) = (np.array(x) for x in load_csv_data(\"data/train.csv\"))\n",
    "(y_test, tX_test, ids_test) = (np.array(x) for x in load_csv_data(\"data/test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w1, loss1_train = least_squares(y_train, tX_train)\n",
    "# loss1_test = MSE(y_test, tX_test, w1)\n",
    "# print(loss1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(tX_train.shape[1]+1)\n",
    "# initial_w = np.random.random(tX_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2, loss2 = least_squares_GD(y_train, tX_train, initial_w, 100, 0.0000001)\n",
    "# loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss2_test = MSE(y_test, tX_test, w2)\n",
    "# loss2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: [0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
      "Step 0, loss is 173286.79514001933\n",
      "pred: [8.47422860e-01 5.57259408e-49 5.19427589e-51 ... 4.26764955e-49\n",
      " 1.67874614e-61 3.94470630e-63]\n",
      "Step 1, loss is nan\n",
      "pred: [8.14164358e-01 8.33614345e-70 3.41573559e-73 ... 5.33837518e-70\n",
      " 3.12554287e-88 6.60774366e-91]\n",
      "Step 2, loss is nan\n",
      "pred: [7.94303155e-001 1.22215546e-090 2.29766740e-095 ... 6.56608363e-091\n",
      " 5.23094403e-115 1.00794578e-118]\n",
      "Step 3, loss is nan\n",
      "pred: [7.86092190e-001 1.77078494e-111 1.52581621e-117 ... 7.98787612e-112\n",
      " 8.15818464e-142 1.40580244e-146]\n",
      "Step 4, loss is nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: RuntimeWarning: divide by zero encountered in log\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: [7.85830519e-001 2.54465356e-132 9.99719428e-140 ... 9.64065868e-133\n",
      " 1.21680760e-168 1.84531368e-174]\n",
      "Step 5, loss is nan\n",
      "pred: [7.90699048e-001 3.63537175e-153 6.48511660e-162 ... 1.15692677e-153\n",
      " 1.76292546e-195 2.32720359e-202]\n",
      "Step 6, loss is nan\n",
      "pred: [7.98804855e-001 5.17165893e-174 4.17721241e-184 ... 1.38264139e-174\n",
      " 2.50435285e-222 2.85651214e-230]\n",
      "Step 7, loss is nan\n",
      "pred: [8.08891652e-001 7.33419298e-195 2.67709629e-206 ... 1.64735377e-195\n",
      " 3.50860951e-249 3.44035928e-258]\n",
      "Step 8, loss is nan\n",
      "pred: [8.20111993e-001 1.03765273e-215 1.70939790e-228 ... 1.95824232e-216\n",
      " 4.86624127e-276 4.08713816e-286]\n",
      "Step 9, loss is nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.37769787e-04,  2.25399091e-02, -9.04829482e-03, -1.11619145e-02,\n",
       "       -7.36415674e-03,  7.97215659e-02,  6.26808175e-02,  7.98589415e-02,\n",
       "       -3.09205415e-04, -3.01542030e-03, -2.33238970e-02, -2.37442595e-04,\n",
       "        5.86808088e-05,  7.98239204e-02, -4.34229928e-03,  8.17527081e-07,\n",
       "       -8.69944304e-07, -6.76834482e-03,  2.38006185e-06, -3.89072270e-06,\n",
       "       -6.17592324e-03,  4.07089878e-06, -3.03420056e-02, -1.63956889e-04,\n",
       "        4.65535120e-02,  5.38708224e-02,  5.38713025e-02,  7.63490218e-02,\n",
       "        7.98463520e-02,  7.98447280e-02, -1.22132527e-02])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx = np.c_[np.ones((y_train.shape[0], 1)), tX_train]\n",
    "w3, loss3 = logistic_regression(y_train, tx, initial_w, 10, 0.0000000001)\n",
    "w3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145855.06257790796"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss3_test = MSE(y_test, tX_test, w3)\n",
    "loss3_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
