{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_train, tx_train_raw, ids_train) = (np.array(x) for x in load_csv_data(\"data/train.csv\"))\n",
    "(y_test, tx_test_raw, ids_test) = (np.array(x) for x in load_csv_data(\"data/test.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noisy_column(x):\n",
    "    \"\"\"Remove columns for which there are more -999 entries than normal entries\"\"\"\n",
    "    return np.array([i for i in x.T if (i == -999).sum() < (i != -999).sum()]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_noise_entries(x):\n",
    "    \"\"\"Replace -999 by mean value of the column (mean computed without any -999)\"\"\"\n",
    "    col_means = [np.mean(col[col!= -999]) for col in x.T]\n",
    "    x_local = x.copy()\n",
    "    for i,col in enumerate(x_local.T):\n",
    "        col[col == -999] = col_means[i]\n",
    "    return x_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outliers(x, mean, std):\n",
    "    outlier_indices = np.zeros() \n",
    "    for feature in range(tx.shape[1]):\n",
    "        row_indices = np.where(np.absolute(tx[:,feature]-mean[feature]) > 3*std[feature])[0]\n",
    "        mask = np.in1d(row_indices, outlier_indices)\n",
    "        outlier_indices = np.hstack((outlier_indices, row_indices[np.where(~mask)[0]]))\n",
    "    \n",
    "    return outlier_indices.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(w, x_test, y_test):\n",
    "    \"\"\"returns accuracy for a specific weight vector\"\"\"\n",
    "    predictions = predict_labels(w, x_test)\n",
    "    num_equal = (predictions == y_test).sum()\n",
    "    return num_equals/y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(x, num_important, degree):\n",
    "\n",
    "    for i, line in enumerate(x):\n",
    "        ones = np.array([1])\n",
    "#         for val in itertools.combinations(line[:num_important], r=degree):\n",
    "#             print(val)\n",
    "#         comb = np.array([np.prod(val) for val in itertools.combinations(line[:num_important], r=degree)])\n",
    "        not_imp = np.array([line**d for d in range(2, degree+1)])\n",
    "        complete = np.concatenate((ones, line, comb, not_imp), axis=None)\n",
    "        print(ones, line, comb, not_imp, complete)\n",
    "        if i == 0:\n",
    "            new = np.zeros((x.shape[0], complete.shape[0]))\n",
    "        new[i] = complete\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([[1,2,3,4],[2,2,2,2],[5,5,5,5], [3,2,3,2], [2,2,4,4]])\n",
    "# np.hstack()\n",
    "augment(t, 3, 3)\n",
    "# t2 = np.array([1,2,3,4,5])\n",
    "# print(np.power(t2, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y, tx, w):\n",
    "    return np.sum(np.power(y - np.dot(tx, w), 2)/(2*len(y)))  # MSE\n",
    "\n",
    "\n",
    "def MAE(y, tx, w):\n",
    "    return np.sum(np.abs(y - np.dot(tx, w)))/len(y)  # MAE\n",
    "\n",
    "\n",
    "def RMSE(y, tx, w):\n",
    "    return np.sqrt(2*MSE(y, tx, w))\n",
    "\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(e)/len(y)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def calculate_gradient_log(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    grad = tx.T.dot(pred - y)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1/(1+np.exp(-t))\n",
    "\n",
    "\n",
    "def NLL(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    loss = y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n",
    "    return np.squeeze(- loss)\n",
    "########################\n",
    "###### ASSIGNMENT ######\n",
    "########################\n",
    "\n",
    "\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        loss = MSE(y, tx, w)\n",
    "        if n_iter % 100 == 0:\n",
    "            print(loss)\n",
    "        w = w - gamma*grad\n",
    "        # print(\"Step {}, loss is   {}\".format(n_iter, loss))\n",
    "    return (w, loss)\n",
    "\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    weights = initial_w\n",
    "    for _ in range(max_iters):\n",
    "        rand_index = np.random.randint(y.shape)\n",
    "        y_batch, tx_batch = y[rand_index], tx[rand_index]\n",
    "        grad = compute_gradient(y_batch, tx_batch, weights)\n",
    "        weights = weights - gamma*grad\n",
    "    loss = MSE(y, tx, weights)\n",
    "    return (weights, loss)\n",
    "\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    loss = MSE(y, tx, w)\n",
    "    return (w, loss)\n",
    "\n",
    "\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    aI = 2 * tx.shape[0] * lambda_ * np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    loss = MSE(y, tx, w)\n",
    "    return (w, loss)\n",
    "\n",
    "\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    pass    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    def sigmoid(t):\n",
    "        \"\"\"apply sigmoid function on t.\"\"\"\n",
    "        return 1/(1+np.exp(-t))\n",
    "\n",
    "    def calculate_gradient(gradient_y, gradient_tx, gradient_w):\n",
    "        \"\"\"compute the gradient of loss.\"\"\"\n",
    "        pred = sigmoid(gradient_tx.dot(gradient_w))\n",
    "        grad = gradient_tx.T.dot(pred - gradient_y)\n",
    "        return grad\n",
    "\n",
    "    def calculate_loss(y, loss_tx, loss_w):\n",
    "        \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "        pred = sigmoid(loss_tx.dot(loss_w))\n",
    "        loss = y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n",
    "        return np.squeeze(- loss)\n",
    "\n",
    "    weights = initial_w\n",
    "    for _ in range(max_iters):\n",
    "        # get loss and update w.\n",
    "        loss = calculate_loss(y, tx, weights)\n",
    "        print(loss)\n",
    "        grad = calculate_gradient(y, tx, weights)\n",
    "        weights = weights - gamma * grad\n",
    "    return (weights, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y, tx, ids) = (np.array(x) for x in load_csv_data(\"data/train.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We quickly saw that the data is split into 4 categories, according to the `PRI_jet_num` column. We split according to these categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT_COL = 22\n",
    "NUM_CATEGORIES = 4\n",
    "rows_per_cat = np.array([np.where(tx[:,CAT_COL] == c)[0] for c in range(NUM_CATEGORIES)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # this represents, for each category, which column contain at least one unknown (-999) value\n",
    "    unknown_cols = [set(np.where(tx[np.where(tx[:, CAT_COL] == c)[0], :] == -999)[1]) for c in range(NUM_CATEGORIES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (rows, cols) in enumerate(zip(rows_per_cat, unknown_cols)):\n",
    "    percentages = np.asarray([len(np.where(tx[rows, i] == -999)[0]) \\\n",
    "                             /len(rows) for i in cols]) * 100\n",
    "    print(\"\\nCATEGORY {}:\".format(i))\n",
    "    for col, perc in zip(cols, percentages):\n",
    "        print(\"Col {} has {}% of unknown\".format(col, perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, for every category, any column (except 0) that has any unknown value has only unknown values. This can be explained by the fact that some fields might not be relevant to a certain category, and thus filled with \"NaN\", or -999\n",
    "\n",
    "From now on, for all categories, we only keep columns with values different that -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in unknown_cols:\n",
    "    cat.remove(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove columns PHI, as they are identically distributed and provide no real meaning.\n",
    "We also remove row 22, as data is now split according to it.\n",
    "As categories 2 and 3 are basically the same, we consider them as identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_cols = [15, 18, 20, 25, 28]\n",
    "columns_to_remove = [np.unique(np.concatenate((list(unknown), [CAT_COL], phi_cols))) for unknown in unknown_cols]\n",
    "columns_to_remove = columns_to_remove[:-1]\n",
    "NUM_CATEGORIES = 3\n",
    "rows_per_cat[2] = np.unique(np.concatenate((rows_per_cat[2],rows_per_cat[3])))\n",
    "rows_per_cat = rows_per_cat[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_by_cat = [tx[rows] for rows in rows_per_cat]\n",
    "y_by_cat = [y[rows] for rows in rows_per_cat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_by_cat = [np.delete(tx_by_cat[cat], np.array(columns_to_remove[cat], dtype=int), axis=1) for cat in range(NUM_CATEGORIES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing remaining NaN by the average value of the column for the category\n",
    "for cat in tx_by_cat:\n",
    "    first_col = cat[:,0]\n",
    "    first_col[first_col == -999] = np.mean(first_col[first_col != -999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from top_features import top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_by_cat[0][:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feat_by_cat, other_feat_by_cat = top_features(tx_by_cat,y_by_cat, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top features for category 0: [1 7 4 9 2]\n",
      "Top features for category 1: [0 2 1 8 9]\n",
      "Top features for category 2: [0 2 4 6 5]\n",
      "Computing features augmentation\n",
      "Building multinomial feature matrix\n",
      "Multinomial partition for (2,5)\n",
      "Multinomial partition for (3,5)\n",
      "Multinomial partition for (4,5)\n",
      "Multinomial partition for (5,5)\n",
      "Multinomial partition for (6,5)\n",
      "Multinomial partition for (7,5)\n",
      "Building multinomial feature matrix\n",
      "Multinomial partition for (2,5)\n",
      "Multinomial partition for (3,5)\n",
      "Multinomial partition for (4,5)\n",
      "Multinomial partition for (5,5)\n",
      "Multinomial partition for (6,5)\n",
      "Multinomial partition for (7,5)\n",
      "Multinomial partition for (8,5)\n",
      "Multinomial partition for (9,5)\n",
      "Multinomial partition for (10,5)\n",
      "Building multinomial feature matrix\n",
      "Multinomial partition for (2,5)\n",
      "Multinomial partition for (3,5)\n",
      "Multinomial partition for (4,5)\n",
      "Multinomial partition for (5,5)\n",
      "Multinomial partition for (6,5)\n",
      "Multinomial partition for (7,5)\n",
      "Multinomial partition for (8,5)\n",
      "Multinomial partition for (9,5)\n",
      "Multinomial partition for (10,5)\n",
      "Computing weights\n",
      "Loading test data\n",
      "Computing predictions\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from implementations import *\n",
    "from proj1_helpers import *\n",
    "from top_features import top_features\n",
    "\n",
    "#load the data\n",
    "(y, tx, ids) = (np.array(x) for x in load_csv_data(\"data/train.csv\"))\n",
    "\n",
    "#We quickly saw that the data is split into 4 categories, \n",
    "# according to the `PRI_jet_num` column. \n",
    "# We split according to these categories\n",
    "\n",
    "CAT_COL = 22\n",
    "NUM_CATEGORIES = 4\n",
    "rows_per_cat = np.array([np.where(tx[:,CAT_COL] == c)[0] for c in range(NUM_CATEGORIES)])\n",
    "\n",
    "# this represents, for each category, which column contain at least one unknown (-999) value\n",
    "unknown_cols = [set(np.where(tx[np.where(tx[:, CAT_COL] == c)[0], :] == -999)[1]) for c in range(NUM_CATEGORIES)]\n",
    "\n",
    "# for i, (rows, cols) in enumerate(zip(rows_per_cat, unknown_cols)):\n",
    "#     percentages = np.asarray([len(np.where(tx[rows, i] == -999)[0]) \\\n",
    "#                              /len(rows) for i in cols]) * 100\n",
    "#     print(\"\\nCATEGORY {}:\".format(i))\n",
    "#     for col, perc in zip(cols, percentages):\n",
    "#         print(\"Col {} has {}% of unknown\".format(col, perc))\n",
    "\n",
    "\n",
    "# Thus, for every category, any column (except 0) that has any unknown value has only unknown values. \n",
    "# This can be explained by the fact that some fields might not be relevant to a certain category, and thus filled with \"NaN\", or -999\n",
    "# From now on, for all categories, we only keep columns with values different that -999\n",
    "\n",
    "# remove col 0 from columns with unknown values.\n",
    "# We'll deal with that later\n",
    "for cat in unknown_cols:\n",
    "    cat.remove(0)\n",
    "\n",
    "#We remove columns PHI, as they are identically distributed and provide no real meaning.\n",
    "# We also remove row 22, as data is now split according to it.\n",
    "# As categories 2 and 3 are basically the same, we consider them as identical\n",
    "\n",
    "phi_cols = [15, 18, 20, 25, 28]\n",
    "columns_to_remove = [np.unique(np.concatenate((list(unknown), [CAT_COL], phi_cols))) for unknown in unknown_cols]\n",
    "columns_to_remove = columns_to_remove[:-1]\n",
    "\n",
    "NUM_CATEGORIES = 3\n",
    "rows_per_cat[2] = np.unique(np.concatenate((rows_per_cat[2],rows_per_cat[3])))\n",
    "rows_per_cat = rows_per_cat[:-1]\n",
    "\n",
    "\n",
    "## Data cleaning\n",
    "tx_by_cat = [tx[rows] for rows in rows_per_cat]\n",
    "y_by_cat = [y[rows] for rows in rows_per_cat]\n",
    "\n",
    "tx_by_cat = [np.delete(tx_by_cat[cat], np.array(columns_to_remove[cat], dtype=int), axis=1) for cat in range(NUM_CATEGORIES)]\n",
    "\n",
    "# Replacing remaining NaN by the average value of the column for the category\n",
    "for cat in tx_by_cat:\n",
    "    first_col = cat[:,0]\n",
    "    first_col[first_col == -999] = np.mean(first_col[first_col != -999])\n",
    "\n",
    "top_feat_by_cat, other_feat_by_cat = top_features(tx_by_cat,y_by_cat, 5)\n",
    "\n",
    "degrees = [7,10,10]\n",
    "augmented_tx_by_cat = []\n",
    "print(\"Computing features augmentation\")\n",
    "for cat, deg, top_feat, other_feat in zip(tx_by_cat, degrees, top_feat_by_cat, other_feat_by_cat):\n",
    "    augmented_tx_by_cat.append(build_multinomial(cat, deg, top_feat, other_feat))\n",
    "    \n",
    "lambdas = [0.001, 0.003, 0.01]\n",
    "print(\"Computing weights\")\n",
    "# weights, _ = apply_ridge_regression(y_by_cat, tx_by_cat, degrees, lambdas, 4, 1, top_feat_by_cat, other_feat_by_cat)\n",
    "weights_by_cat = []\n",
    "for lamb, tx_cat, y_cat in zip(lambdas, augmented_tx_by_cat, y_by_cat):\n",
    "    weights_by_cat.append(ridge_regression(y_cat, tx_cat, lamb))\n",
    "\n",
    "print(\"Loading test data\")\n",
    "(y_test, tx_test, ids_test) = (np.array(x) for x in load_csv_data(\"data/test.csv\"))\n",
    "\n",
    "print(\"Computing predictions\")\n",
    "rows_per_cat_test = [\n",
    "    np.array([np.where(tx[:,CAT_COL] == 0)[0]]),\n",
    "    np.array([np.where(tx[:,CAT_COL] == 1)[0]]),\n",
    "    np.array([np.where(tx[:,CAT_COL] >= 2)[0]])\n",
    "]\n",
    "\n",
    "y_predict = np.zeros(tx_test.shape[0])\n",
    "\n",
    "category_col = tx_test[:,CAT_COL]\n",
    "#Merge categories 2 and 3 into one single category (2)\n",
    "cat3_idx = np.where(category_col == 3)\n",
    "category_col[cat3_idx] = 2*np.ones(len(cat3_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: value array of shape (99913,) could not be broadcast to indexing result of shape (227458,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-78d85d31ae14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_by_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0my_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory_col\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_by_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugmented_tx_by_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# [weights_by_cat[i][0] for i in range(3)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# create_csv_submission(ids_test, y_predict, 'submission.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shape mismatch: value array of shape (99913,) could not be broadcast to indexing result of shape (227458,)"
     ]
    }
   ],
   "source": [
    "for i in range(len(weights_by_cat)):\n",
    "    y_predict[np.where(category_col == i)[0]] =  predict_labels(weights_by_cat[i][0], augmented_tx_by_cat[i])\n",
    "# [weights_by_cat[i][0] for i in range(3)]\n",
    "\n",
    "# create_csv_submission(ids_test, y_predict, 'submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568238,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_col.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
