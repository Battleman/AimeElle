{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_train, tx_train_raw, ids_train) = (np.array(x) for x in load_csv_data(\"data/train.csv\"))\n",
    "(y_test, tx_test_raw, ids_test) = (np.array(x) for x in load_csv_data(\"data/test.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noisy_column(x):\n",
    "    \"\"\"Remove columns for which there are more -999 entries than normal entries\"\"\"\n",
    "    return np.array([i for i in x.T if (i == -999).sum() < (i != -999).sum()]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_noise_entries(x):\n",
    "    \"\"\"Replace -999 by mean value of the column (mean computed without any -999)\"\"\"\n",
    "    col_means = [np.mean(col[col!= -999]) for col in x.T]\n",
    "    x_local = x.copy()\n",
    "    for i,col in enumerate(x_local.T):\n",
    "        col[col == -999] = col_means[i]\n",
    "    return x_local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(w, x_test, y_test):\n",
    "    \"\"\"returns accuracy for a specific weight vector\"\"\"\n",
    "    predictions = predict_labels(w, x_test)\n",
    "    num_equal = (predictions == y_test).sum()\n",
    "    return num_equals/y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(x, num_important, degree):\n",
    "\n",
    "    for i, line in enumerate(x):\n",
    "        ones = np.array([1])\n",
    "        for val in itertools.combinations(line[:num_important], r=degree):\n",
    "            print(val)\n",
    "        comb = np.array([np.prod(val) for val in itertools.combinations(line[:num_important], r=degree)])\n",
    "        not_imp = np.array([line**d for d in range(2, degree+1)])\n",
    "        complete = np.concatenate((ones, line, comb, not_imp), axis=None)\n",
    "        print(ones, line, comb, not_imp, complete)\n",
    "        if i == 0:\n",
    "            new = np.zeros((x.shape[0], complete.shape[0]))\n",
    "        new[i] = complete\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 3)\n",
      "[1] [1 2 3 4] [6] [[ 1  4  9 16]\n",
      " [ 1  8 27 64]] [ 1  1  2  3  4  6  1  4  9 16  1  8 27 64]\n",
      "(2, 2, 2)\n",
      "[1] [2 2 2 2] [8] [[4 4 4 4]\n",
      " [8 8 8 8]] [1 2 2 2 2 8 4 4 4 4 8 8 8 8]\n",
      "(5, 5, 5)\n",
      "[1] [5 5 5 5] [125] [[ 25  25  25  25]\n",
      " [125 125 125 125]] [  1   5   5   5   5 125  25  25  25  25 125 125 125 125]\n",
      "(3, 2, 3)\n",
      "[1] [3 2 3 2] [18] [[ 9  4  9  4]\n",
      " [27  8 27  8]] [ 1  3  2  3  2 18  9  4  9  4 27  8 27  8]\n",
      "(2, 2, 4)\n",
      "[1] [2 2 4 4] [16] [[ 4  4 16 16]\n",
      " [ 8  8 64 64]] [ 1  2  2  4  4 16  4  4 16 16  8  8 64 64]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1.,   1.,   2.,   3.,   4.,   6.,   1.,   4.,   9.,  16.,   1.,\n",
       "          8.,  27.,  64.],\n",
       "       [  1.,   2.,   2.,   2.,   2.,   8.,   4.,   4.,   4.,   4.,   8.,\n",
       "          8.,   8.,   8.],\n",
       "       [  1.,   5.,   5.,   5.,   5., 125.,  25.,  25.,  25.,  25., 125.,\n",
       "        125., 125., 125.],\n",
       "       [  1.,   3.,   2.,   3.,   2.,  18.,   9.,   4.,   9.,   4.,  27.,\n",
       "          8.,  27.,   8.],\n",
       "       [  1.,   2.,   2.,   4.,   4.,  16.,   4.,   4.,  16.,  16.,   8.,\n",
       "          8.,  64.,  64.]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([[1,2,3,4],[2,2,2,2],[5,5,5,5], [3,2,3,2], [2,2,4,4]])\n",
    "# np.hstack()\n",
    "augment(t, 3, 3)\n",
    "# t2 = np.array([1,2,3,4,5])\n",
    "# print(np.power(t2, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y, tx, w):\n",
    "    return np.sum(np.power(y - np.dot(tx, w), 2)/(2*len(y)))  # MSE\n",
    "\n",
    "\n",
    "def MAE(y, tx, w):\n",
    "    return np.sum(np.abs(y - np.dot(tx, w)))/len(y)  # MAE\n",
    "\n",
    "\n",
    "def RMSE(y, tx, w):\n",
    "    return np.sqrt(2*MSE(y, tx, w))\n",
    "\n",
    "\n",
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    e = y - tx.dot(w)\n",
    "    grad = -tx.T.dot(e)/len(y)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def calculate_gradient_log(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    grad = tx.T.dot(pred - y)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1/(1+np.exp(-t))\n",
    "\n",
    "\n",
    "def NLL(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    loss = y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n",
    "    return np.squeeze(- loss)\n",
    "########################\n",
    "###### ASSIGNMENT ######\n",
    "########################\n",
    "\n",
    "\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        grad = compute_gradient(y, tx, w)\n",
    "        loss = MSE(y, tx, w)\n",
    "        if n_iter % 100 == 0:\n",
    "            print(loss)\n",
    "        w = w - gamma*grad\n",
    "        # print(\"Step {}, loss is   {}\".format(n_iter, loss))\n",
    "    return (w, loss)\n",
    "\n",
    "\n",
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    weights = initial_w\n",
    "    for _ in range(max_iters):\n",
    "        rand_index = np.random.randint(y.shape)\n",
    "        y_batch, tx_batch = y[rand_index], tx[rand_index]\n",
    "        grad = compute_gradient(y_batch, tx_batch, weights)\n",
    "        weights = weights - gamma*grad\n",
    "    loss = MSE(y, tx, weights)\n",
    "    return (weights, loss)\n",
    "\n",
    "\n",
    "def least_squares(y, tx):\n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    loss = MSE(y, tx, w)\n",
    "    return (w, loss)\n",
    "\n",
    "\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    aI = 2 * tx.shape[0] * lambda_ * np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    loss = MSE(y, tx, w)\n",
    "    return (w, loss)\n",
    "\n",
    "\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    pass    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    def sigmoid(t):\n",
    "        \"\"\"apply sigmoid function on t.\"\"\"\n",
    "        return 1/(1+np.exp(-t))\n",
    "\n",
    "    def calculate_gradient(gradient_y, gradient_tx, gradient_w):\n",
    "        \"\"\"compute the gradient of loss.\"\"\"\n",
    "        pred = sigmoid(gradient_tx.dot(gradient_w))\n",
    "        grad = gradient_tx.T.dot(pred - gradient_y)\n",
    "        return grad\n",
    "\n",
    "    def calculate_loss(y, loss_tx, loss_w):\n",
    "        \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "        pred = sigmoid(loss_tx.dot(loss_w))\n",
    "        loss = y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))\n",
    "        return np.squeeze(- loss)\n",
    "\n",
    "    weights = initial_w\n",
    "    for _ in range(max_iters):\n",
    "        # get loss and update w.\n",
    "        loss = calculate_loss(y, tx, weights)\n",
    "        print(loss)\n",
    "        grad = calculate_gradient(y, tx, weights)\n",
    "        weights = weights - gamma * grad\n",
    "    return (weights, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_train = augment(tx_train_raw)\n",
    "tx_test = augment(tx_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(tx_train.shape[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
